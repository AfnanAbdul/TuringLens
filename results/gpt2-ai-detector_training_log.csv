	loss	grad_norm	learning_rate	epoch	step	eval_loss	eval_runtime	eval_samples_per_second	eval_steps_per_second	train_runtime	train_samples_per_second	train_steps_per_second	total_flos	train_loss
0	0.2243	0.007917703129351139	1.8693322879916372e-05	0.1960015680125441	500									
1	0.0838	2.974534749984741	1.738664575983275e-05	0.3920031360250882	1000									
2	0.0805	0.005135903600603342	1.607996863974912e-05	0.5880047040376323	1500									
3	0.0726	78.45806121826172	1.477329151966549e-05	0.7840062720501764	2000									
4	0.0409	0.08057599514722824	1.3466614399581866e-05	0.9800078400627205	2500									
5				1.0	2551	0.2928582429885864	156.5432	27.928	3.494					
6	0.0209	0.0007819598540663719	1.2159937279498238e-05	1.1760094080752646	3000									
7	0.0123	0.0004991399473510683	1.085326015941461e-05	1.3720109760878088	3500									
8	0.0239	0.0017453917535021901	9.546583039330982e-06	1.5680125441003527	4000									
9	0.0096	0.000422898301621899	8.239905919247355e-06	1.764014112112897	4500									
10	0.0167	51.72990417480469	6.933228799163727e-06	1.9600156801254411	5000									
11				2.0	5102	0.3414234519004822	158.7383	27.542	3.446					
12	0.0108	0.00031445128843188286	5.6265516790801e-06	2.156017248137985	5500									
13	0.007	0.0008616558043286204	4.319874558996473e-06	2.352018816150529	6000									
14	0.0049	0.001250582397915423	3.013197438912845e-06	2.548020384163073	6500									
15	0.0015	0.0001700307911960408	1.7065203188292174e-06	2.7440219521756175	7000									
16	0.0001	0.00011479052773211151	3.9984319874559e-07	2.9400235201881615	7500									
17				3.0	7653	0.2529148757457733	153.5684	28.469	3.562					
18				3.0	7653					9388.611	6.519	0.815	3998036256620544.0	0.03984930455412311
